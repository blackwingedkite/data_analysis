{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f5f2b1",
   "metadata": {},
   "source": [
    "## B10703049 財金二 柯宥圻\n",
    "### Homework 5 Question 1\n",
    "The first question ask us to practice and discuss the results of LR, KNN and SVM on the ORLface dataset. The preprocessing is performed several times, so I won't discuss it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114408f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = []\n",
    "for i in range(1, 41):\n",
    "    for j in range(1,11):\n",
    "        image_dir = f\"C:/Users/user/Desktop/課程資料/1a DA/ORL faces/{i}_{j}.png\"\n",
    "        img = Image.open(image_dir)\n",
    "        img_array = np.asarray(img)\n",
    "        data.append(img_array.flatten())\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d94e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = [0,1,1,1,1,1,1,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1]\n",
    "genders = []\n",
    "for i in gender:\n",
    "    for _ in range(10):\n",
    "        genders.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18af09",
   "metadata": {},
   "source": [
    "After finishing the data preprocessing process, we use train_test_split to split the training set and testing set, and we choose a random state to ensure that the training data and testing data be the same whenever the code is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1db1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, genders, test_size=0.2, random_state=212)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f32714",
   "metadata": {},
   "source": [
    "The first classfier is logistic regression, we take the penalty l2 to get better results, and define the max iteration in order to avoid the warning message. The message tells you that the model reached the upper limit of iterations during training. This may be because the model is experiencing convergence issues, it may be due to insufficient data or more tuning is required. Therefore, I should define the max iteration to get rid of these issues. We can see that the Accuracy is rather high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1c7bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Accuracy: 97.50%\n"
     ]
    }
   ],
   "source": [
    "'''Classfier 1: Logistic regression'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_LR = LogisticRegression(max_iter=5000,penalty='l2',random_state=0)\n",
    "classifier_LR.fit(X_train, y_train)\n",
    "accuracy = classifier_LR.score(X_test, y_test)\n",
    "print(\"Logistic regression Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbefc1",
   "metadata": {},
   "source": [
    "The second classfier is KNN. we take n_neighbors be 5. The accuracy is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1cf1151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 98.75%\n"
     ]
    }
   ],
   "source": [
    "'''Classfier 2: KNN'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier_KNN.fit(X_train, y_train)\n",
    "accuracy = classifier_KNN.score(X_test, y_test)\n",
    "print(\"KNN Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a9fb5",
   "metadata": {},
   "source": [
    "The third classfier is SVC, we use kernal=linear to this data. There are many kernal to choose in SVC model, but in this case, linear model is great enough and better describe the ORLFace dataset. We choose a random state to ensure the same result, again. We can see the Accuracy is as high as the LR methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f6063e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 97.50%\n"
     ]
    }
   ],
   "source": [
    "'''Classfier 3: SVM(linear)'''\n",
    "from sklearn.svm import SVC\n",
    "classifier_SVC = SVC(kernel='linear', random_state=622)\n",
    "classifier_SVC.fit(X_train, y_train)\n",
    "accuracy = classifier_SVC.score(X_test, y_test)\n",
    "print(\"SVM Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef0bf6",
   "metadata": {},
   "source": [
    "### Homework 5 Question 2\n",
    "In this problem, we have to consider the parsimonious principle in modelling. Therefore, we should take a few variables to get as close as possible to the result in EX1. Since the details are not defined, I use my own way to define the meaning of \"getting closer\".\n",
    "\n",
    "When building a model, feature selection can be used to select the most important variables, thereby simplifying the model.According to the notions in HW1, we can use correlation coefficient analysis and tree model for feature selection. Specific steps are as follows:\n",
    "\n",
    "1. Convert data and genders to a DataFrame (using pandas)\n",
    "2. Computes the correlation coefficient between each feature and the target variable.\n",
    "3. Select features with high correlation coefficients.In this case, we selected the features with the absolute value of the correlation coefficient greater than 0.1 as the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c16930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df['genders'] = genders\n",
    "correlations = df.corr()['genders'].drop('genders')\n",
    "selected_features = correlations[abs(correlations) > 0.1].index.tolist()\n",
    "X = df[selected_features]\n",
    "y = df['genders']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7235f7",
   "metadata": {},
   "source": [
    "Then in these selected features, we try to calculate the importance of each feature using a decision tree model. After that, we select features with higher importance, agian. We selected features with feature importance greater than 0.1 as selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7b07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1617, 2301]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "importance = model.feature_importances_\n",
    "selected_features = X.columns[importance > 0.1].tolist()\n",
    "X = df[selected_features]\n",
    "y = df['genders']\n",
    "\n",
    "print(selected_features)\n",
    "print(len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d73838",
   "metadata": {},
   "source": [
    "We can see only two features are selected, which are really a few. We can use the same ways in EX1 to check the accuracy of each classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "828861b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce08f6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature_Selection - Logistic regression Accuracy: 95.00%\n"
     ]
    }
   ],
   "source": [
    "'''Classfier 1: Logistic regression'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_LR = LogisticRegression(max_iter=5000,penalty='l2',random_state=0)\n",
    "classifier_LR.fit(X_train, y_train)\n",
    "accuracy = classifier_LR.score(X_test, y_test)\n",
    "print(\"Feature_Selection - Logistic regression Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0f94bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 96.25%\n"
     ]
    }
   ],
   "source": [
    "'''Classfier 2: KNN'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier_KNN.fit(X_train, y_train)\n",
    "accuracy = classifier_KNN.score(X_test, y_test)\n",
    "print(\"KNN Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd8fe798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 93.75%\n"
     ]
    }
   ],
   "source": [
    "'''Classfier 3: SVM(linear)'''\n",
    "from sklearn.svm import SVC\n",
    "classifier_SVC = SVC(kernel='linear', random_state=622)\n",
    "classifier_SVC.fit(X_train, y_train)\n",
    "accuracy = classifier_SVC.score(X_test, y_test)\n",
    "print(\"SVM Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf42e67",
   "metadata": {},
   "source": [
    "We can see that although there are only two variables, the accuracy are still high. Precisely, the accuracy of each classifiers just fall by 2-3%, which still remain great results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ccae8",
   "metadata": {},
   "source": [
    "### Homework 5 Question 3\n",
    "The question ask us to looj for the multiclass classfier in LR, Knn and SVM. Apply them to analyze AutoMPG and discuss the results. The target is to classify the \"origin\" of the car and \"mpg\" can be included in the X.\n",
    "\n",
    "The \"origin\" variable is a nominal scales, which are 1, 2, 3, depending on the car's origin countries or regions. Therefore, we can use multiple classfier to classify the orgin with 7 distinct independent variables, including MPG.\n",
    "\n",
    "In the case, I have found several multiple classifier to solve the problem the codes are shown below. We skip the discussion of data preprocessing, which is spoken before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ff73e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"C:/Users/user/Desktop/autompg.csv\")\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Define the independent and dependent variables\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=121)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3274903",
   "metadata": {},
   "source": [
    "I thought that performing data normalization can help improve the training performance of the model and reduce the possibility of convergence problems. In Python, you can use the StandardScaler from the Scikit-learn library for data normalization.\n",
    "\n",
    "However, after testing, I found the result become worse in all classfiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c06c1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standardize (all worsen)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f324d9db",
   "metadata": {},
   "source": [
    "In the code above, we first create a StandardScaler object, then use the fit_transform method to normalize the training data and store it in X_train_scaled. Next, we apply the same normalizer object to the test data using the transform method and store it in X_test_scaled.\n",
    "\n",
    "I think the reason why it become worse is that we use some inappropriate normalization method: If the normalization method chosen is not suitable for a particular dataset or model, it may lead to poor performance. In the dataset, it should not be useful to standarization the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caabf1a",
   "metadata": {},
   "source": [
    "In LR method, I found that newton-cg is a great solver. All other remains the same, we can see that the accuracy is 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c795ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy: 86.08%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Logistic regression\"\"\"\n",
    "model = LogisticRegression(multi_class='multinomial', solver='newton-cg',max_iter=5000,penalty='l2')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"LogisticRegression Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210e033",
   "metadata": {},
   "source": [
    "The KNN method is 78%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe69c372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 78.48%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"KNN\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"KNN Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16881043",
   "metadata": {},
   "source": [
    "The SVM method is 89%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e1b73e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 88.61%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"SVM\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"SVM Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
